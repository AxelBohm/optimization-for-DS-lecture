\documentclass{beamer}

\input{../../shared_slides.tex}


% Reference: https://arxiv.org/abs/1912.13213

\title{Online Optimization}
\date{\today}

\begin{document}
\maketitle
\frame{\tableofcontents}

\section{Introduction}

\begin{frame}
  \frametitle{What is online Learning}
Consider the repeated game: In each round $t=1,\dots,T$
\begin{itemize}
  \item an adversary chooses a (secret) number in $y_t \in [0,1]$
  \item you guess the real number, choosing $x_t \in [0,1]$;
  \item you pay the squared difference ${(x_t-y_t)}^2$.
\end{itemize}
\textbf{Task:} guess a sequence of numbers as precisely as possible.
To be a game, we now have to decide what is the ``winning condition''. Let's see what makes sense to consider as winning condition.
\textbf{Question:} How to measure success?
\end{frame}


\begin{frame}
  \frametitle{Adversary plays i.i.d.}
  Consider: Adversary number are drawn from a fixed distribution (with mean $\mu$ and Variance $\sigma^2$).
  If we knew the distribution, we could pick the mean and pay in expectation $\sigma^2T$ (optimal).
  \begin{equation}
    \label{eq:stoch_regret}
    \E_Y\left[\sum_{t=1}^T (x_t - Y)^2\right] - \sigma^2 T,
  \end{equation}
  or equivalently considering the average
  \begin{equation}
    \label{eq:av_stoch_regret}
    \frac{1}{T}\E_Y\left[\sum_{t=1}^T (x_t - Y)^2\right] - \sigma^2~.
  \end{equation}
\end{frame}


\begin{frame}
  \frametitle{Minimizing Regret}
  Let's rewrite a bit more general
  \[
    \E\left[\sum_{t=1}^T (x_t - Y)^2\right] - \min_{ x \in [0,1]} \ \E\left[\sum_{t=1}^T (x-Y)^2\right]~.
  \]
  ($\sigma^2T$ was just the payoff of the best possible strategy)

  Finally:
  \begin{itemize}
    \item remove the assumption on how the data is generated,
    \item consider any arbitrary sequence of $y_t$
   (can remove the expectation).
  \end{itemize}

  \[
    R_T:=\sum_{t=1}^T (x_t - y_t)^2 - \min_{x \in [0,1]} \ \sum_{t=1}^T (x - y_t)^2
  \]
  Called \textbf{Regret}, because it measures how much the algorithm regrets for not sticking on all the rounds to the optimal choice in hindsight.
\end{frame}


\begin{frame}
  \frametitle{General loss functions}
  Online Learning is the study of algorithms to minimize the \emph{regret} over a sequence of loss functions with respect to an arbitrary competitor $u \in V\subseteq \R^d$:
  \[
    R_T(u):=\sum_{t=1}^T \ell_t(x_t) - \sum_{t=1}^T \ell_t(u)~.
  \]
  Regret framework allows to
  \begin{itemize}
    \item reformulate problems in ML and optimization as similar games.
    \item analyze situations in which the data are not i.i.d.\, yet want to guarantee that the algorithm is ``learning'' something.
  \end{itemize}

  For example, online learning can be used to analyze
  \begin{itemize}
    \item Click prediction problems;
    \item Routing on a network;
    \item Convergence to equilibrium of \textit{repeated} games.
  \end{itemize}
  It can \emph{also} be used to analyze stochastic algorithms, e.g., Stochastic Gradient Descent, but the adversarial nature of the analysis might give you suboptimal results.
\end{frame}


\begin{frame}
  \frametitle{Back to the numbers game}
  % Now, let's try to design a strategy to make the regret provably sublinear in time, \emph{regardless of how the adversary chooses the numbers}.
  Consider: the \textbf{best strategy in hindsight}, that is argmin of the second term of the regret. Clearly
  \[
    x^\star_T := \argmin_{x \in [0,1]} \ \sum_{t=1}^T (x - y_t)^2 = \frac{1}{T} \sum_{t=1}^T y_t~.
  \]
  \begin{itemize}
    \item Don't know the future: $x^*_T$ is not an option in each round
    \item But do know the past. in each round: best number over the past.
    \item not because we expect the future to be like the past (not true)
    \item optimal guess should not change too much between rounds (so we can try to ``track'' it over time)
  \end{itemize}
  Hence, on each round $t$ our strategy is to guess $x_t = x_{t-1}^\star=\frac{1}{t-1} \sum_{i=1}^{t-1} y_i$, called \textbf{Follow-the-Leader} (FTL).
\end{frame}

\section{Strategies}%
\label{sec:}

\begin{frame}
  \frametitle{Follow the leader}
  Now we show that this strategy will allow us to win the game.
  \begin{lemma}
    \label{lemma:be_leader}
    Let $V \subseteq \R^d$ and $\ell_t :V \rightarrow \R$ an arbitrary sequence of loss functions.
    Denote by $x^\star_t$ a minimizer of the cumulative losses over the previous $t$ rounds in $V$. Then, we have
    \[
      \sum_{t=1}^T \ell_t(x^\star_{t}) \leq \sum_{t=1}^T \ell_t(x^\star_{T})~.
    \]
  \end{lemma}
  %
  \begin{proof}
    We prove it by induction over $T$. The base case is
    \[
      \ell_1(x^\star_1) \leq \ell_1(x^\star_{1}),
    \]
    that is trivially true.
    Now, for $T\geq2$, we assume that $\sum_{t=1}^{T-1} \ell_t(x^\star_{t}) \leq \sum_{t=1}^{T-1} \ell_t(x^\star_{T-1})$ is true and we must prove the stated inequality, that is
    \[
      \sum_{t=1}^T \ell_t(x^\star_{t}) \leq \sum_{t=1}^T \ell_t(x^\star_{T})~.
    \]
    This inequality is equivalent to
    \begin{equation}
      \label{eq:lemma1}
      \sum_{t=1}^{T-1} \ell_t(x^\star_{t}) \leq \sum_{t=1}^{T-1} \ell_t(x^\star_{T}),
    \end{equation}
    where we removed the last element of the sums because they are the same.
    Now observe that
    \[
      \sum_{t=1}^{T-1} \ell_t(x^\star_{t}) \leq \sum_{t=1}^{T-1} \ell_t(x^\star_{T-1}),
    \]
    by induction hypothesis, and
    \[
      \sum_{t=1}^{T-1} \ell_t(x^\star_{T-1}) \leq \sum_{t=1}^{T-1} \ell_t(x^\star_{T})
    \]
    because $x^\star_{T-1}$ is a minimizer of the left hand side in $V$ and $x^\star_{T} \in V$.
    Chaining these two inequalities, we have that \eqref{eq:lemma1} is true, and so the theorem is proven.
  \end{proof}
  Basically, the above lemma quantifies the idea the knowing the future and being adaptive to it is typically better than not being adaptive to it.

\end{frame}

\begin{frame}
  \frametitle{Follow the leader II}
  \begin{theorem}
    Let $y_t \in [0,1]$ for $t=1,\dots,T$ an arbitrary sequence of numbers. Let the algorithm's output $x_t=x_{t-1}^\star:=\frac{1}{t-1}\sum_{i=1}^{t-1} y_i$. Then, we have
    \[
      R_T = \sum_{t=1}^T (x_t - y_t)^2 - \min_{x \in [0,1]} \ \sum_{t=1}^T (x - y_t)^2 \leq 4 + 4\ln T~.
    \]
  \end{theorem}
  %
  \begin{proof}
    Exercise.
  \end{proof}

\end{frame}


\begin{frame}
  \frametitle{Failure of FTL}
    Let $V = [-1,1]$ and consider the sequence of losses $\ell_t(x) = z_t x + i_V(x)$, where $z_1 = -0.5$
    \begin{equation}
      z_t = \begin{cases}
       1, & \text{$t$ even} \\
       -1, & \text{$t$ odd}
      \end{cases}
    \end{equation}
    Predictions of FTL will be $x_t = 1$ for $t$ even and $x_t = -1$ for $t$ odd.
    Cumulative loss of the FTL algorithm will be $T$ while the cumulative loss of the fixed solution $u = 0$ is 0. Thus, the regret of FTL is $T$.

    \textbf{Outlook:}
    \begin{itemize}
      \item Follow the \emph{regularized} leader
      \item Online gradient descent % will be optimal
    \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Weighted majority algorithm}
  Consider the \emph{learning from experts} scenario.
  Experts $= 1, \dots, n$. Decision: ``Yes'' or ``No''.
  \begin{equation}
    f_t(x_t) = \begin{cases}
      1 & \text{if wrong}\\
      0 & otherwise
    \end{cases}
  \end{equation}

  \begin{enumerate}
    \item $w_1(i) = 1$ for all $i = 1, \dots, n$
    \item for $t=1, \dots, T$
      \begin{enumerate}
      \item compare weights $\sum_{i\in YES} w_t(i)$ vs. $\sum_{i\in NO} w_t(i)$
      \item choose Yes or No depending on above comparison
      \item observe feedback
      \item update weights:
        \begin{equation}
          w_{t+1}(i) = \begin{cases}
              w_t(i) & \text{if Expert $i$ was right} \\
              (1-\alpha)w_t(i) & \text{if Expert $i$ made a mistake}
          \end{cases}
        \end{equation}

      \end{enumerate}
  \end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Weighted majority algorithm II}
  \begin{theorem}
    Let $M_t$ be the number mistakes we make after $t$ attempts and
    $m_t(i) = \#$ the number of mistakes expert $i$ made...
    Then,
    \begin{equation}
      M_T \le 2(1+\alpha) m_T(i) + 2 \frac{\log(n)}{\alpha}
    \end{equation}
    Also
    \begin{equation}
      M_T -m_T(i^*) = R_T
    \end{equation}

  \end{theorem}
\end{frame}

\begin{frame}
  \frametitle{Proof of the Theorem}
  We always have $ \Vert w_{t+1} \Vert_1 \le \Vert w_t \Vert_1$. Also, if we made a mistake, then
  \begin{equation}
    \begin{aligned}
      \Vert w_{t+1} \Vert_1 &\le \frac12 \Vert w_t \Vert_1 + \frac12 \Vert_1 w_t \Vert (1-\alpha) \\
      &= \Vert w_t \Vert_1 (1-\alpha/2) \\
      &\le \Vert w_1 \Vert_1 (1-\alpha/2)^{M_t} \\
      &= n (1-\alpha/2)^{M_t} \\
    \end{aligned}
  \end{equation}
  Next
  \begin{equation}
    w_{t+1}(i) = (1-\alpha)^{m_t(i)} \le \Vert w_{t+1} \Vert_1
  \end{equation}
  Combining the above two yields
  \begin{equation}
    {(1-\alpha)}^{m_t(i)} \le n (1-\alpha/2)^{M_t}
  \end{equation}
  and
  \begin{equation}
    m_t(i) \log(1-\alpha) \le \log(n) + M_T \log(1-\alpha/2).
  \end{equation}
\end{frame}

% the rest of the proof is only technical.
%
\begin{frame}
  \frametitle{remainder of the proof}
  use the fact that for $x\in (0,\frac12)$
  \begin{equation}
    -x -x^2 \le \log(1-x) \le -x
  \end{equation}
  to deduce that
  \begin{equation}
    \begin{aligned}
    - m_t(i) (\alpha+\alpha^2) &\le \log(n) - M_T \frac{\alpha}{2}
    -2 m_t(i) (1+\alpha) &\le \frac{2}{\alpha}\log(n) - M_T
    \end{aligned}
  \end{equation}
  which yields
  \begin{equation}
    M_T -  \le 2 m_t(i) (1+\alpha) + \frac{2}{\alpha}\log(n) -
  \end{equation}

\end{frame}


\begin{frame}
  \frametitle{Randomized Weighted Majority}

  Instead of picking the optinion of the (weighted) majority, we only do so with a \textbf{probability}.

  \begin{enumerate}
    \item $w_1(i) = 1$ for all $i = 1, \dots, n$ and $\alpha \in (0, 1)$
    \item for $t=1, \dots, T$
      \begin{enumerate}
      \item compute $p_t(i) = w_t(i)/\Vert w_t \Vert_1$
      \item choose expert $i$ with probability $p_t(i)$
      \item observe feedback
      \item update weights:
        \begin{equation}
          w_{t+1}(i) = \begin{cases}
              w_t(i) & \text{if expert $i$ was right} \\
              (1-\alpha)w_t(i) & \text{if expert $i$ made a mistake}
          \end{cases}
        \end{equation}

      \end{enumerate}
  \end{enumerate}

  \onslide<2->{%
  \emph{Comment:} Randomizing algorithms typically improves the (worst case) analysis.

  }
\end{frame}


\begin{frame}
  \frametitle{Randomized Weighted Majority contd.}
  As before:\\
    $M_t= \#$ of mistakes we make after $t$ attempts and
    $m_t(i) = \#$ of mistakes expert $i$ made.
    \begin{theorem}
      \begin{equation}
        \E[M_T] \le (1+\alpha)m_T(i) + \frac{\log(n)}{\alpha}
      \end{equation}
    \end{theorem}
    \textbf{Improved constants!}
\end{frame}

\begin{frame}
  \frametitle{proof of randomized WMA}

\end{frame}


\begin{frame}
  \frametitle{Multiplicative Weights Algorithm}
  Before: Loss $l_t$ was $0$ or $1$
  Now: General loss functions $\bm{\ell}_t= (\ell_t(1), \dots, \ell_t(n))$ with $\ell_t(i) \in [-1, 1]$
  \begin{enumerate}
    \item $w_1(i) = 1$ for all $i = 1, \dots, n$ and $\alpha \in (0, 1)$
    \item for $t=1, \dots, T$
      \begin{enumerate}
        \item compute $p_t(i) = w_t(i)/\Vert w_t \Vert_1$
        \item choose expert $i$ with probability $p_t(i)$
        \item observe loss $\bm{\ell}_t= (\ell_t(1), \dots, \ell_t(n))$
        \item update weights:
          \begin{equation}
            w_{t+1}(i) = (1-\alpha \ell_t(i)) w_t(i)
          \end{equation}
      \end{enumerate}
  \end{enumerate}

  Note that
  \begin{equation}
    \langle \bm{p}_t, \bm{\ell}_t \rangle = p_t(1)\ell_t(1) + \cdots + p_t(n)\ell_t(n) = \E_i[\ell_t(i)]
  \end{equation}
  gives expected loss of round $t$.
\end{frame}

\begin{frame}
  \frametitle{Multiplicative Weights Algorithm [contd]}

  \begin{theorem}
    if $\ell_t(i) \in [-1, 1]$ and $\alpha < \frac12$, then \textbf{MWA} guarantees

    \begin{equation}
      \sum_{t=1}^{T} \langle \bm{p}_t, \bm{\ell}_t \rangle  - \sum_{t=1}^{T} \ell_t(i) \le \alpha \sum_{t=1}^{T} \vert \ell_t(i) \vert + \frac{\log(n)}{\alpha} \quad \forall i
    \end{equation}

  \end{theorem}

\end{frame}


\begin{frame}
  \frametitle{Hedge Algorithm}
  \begin{enumerate}
    \item $w_1(i) = 1$ for all $i = 1, \dots, n$ and $\alpha \in (0, 1)$
    \item for $t=1, \dots, T$
      \begin{enumerate}
        \item compute $p_t(i) = w_t(i)/\Vert w_t \Vert_1$
        \item choose expert $i$ with probability $p_t(i)$
        \item observe loss $\bm{\ell}_t= (\ell_t(1), \dots, \ell_t(n))$
        \item update weights:
          \begin{equation}
            w_{t+1}(i) =  w_t(i) \textcolor{red}{e^{-\alpha \ell_t(i)}}
          \end{equation}
      \end{enumerate}
  \end{enumerate}
  \vfill
  Note:
  \begin{equation}
    e^{-x} \approx 1-x
  \end{equation}

\end{frame}


\begin{frame}
  \frametitle{Hedge Algorithm [contd]}

  \begin{theorem}
    If $\ell_t(i) \in [-1, 1]$ and $\alpha < \frac12$, then \textbf{Hedge Alg.} guarantees

    \begin{equation}
      \sum_{t=1}^{T} \langle \bm{p}_t, \bm{\ell}_t \rangle  - \sum_{t=1}^{T} \ell_t(i) \le \alpha \sum_{t=1}^{T} \langle p_t, \ell_t \rangle^2 + \frac{\log(n)}{\alpha} \quad \forall i.
    \end{equation}
  \end{theorem}
  \onslide<2->{%

  \textbf{Observe:} Iteration $t$ is just
  \begin{equation}
    \begin{aligned}
      w_{t+1}(i) &= w_t(i) e^{-\alpha \ell_t(i)} \\
      p_{t+1}(i) &= \frac{w_{t+1}(i)}{\Vert w_{t+1}\Vert_1}
    \end{aligned}
  \end{equation}
  \textcolor{red}{Online mirror descent!}
  \textcolor{red}{(KL-divergence setting:)}
  \begin{equation}
    h(x) = \sum_{i}^{} x(i) \log(x(i))
  \end{equation}
  }

\end{frame}

\end{document}
