\documentclass[aspectration=169]{beamer}
\usepackage[utf8]{inputenc}

\input{../../shared_slides.tex}

\newcounter{sauvegardeenumi}
\newcommand{\asuivre}{\setcounter{sauvegardeenumi}{\theenumi}}
\newcommand{\suite}{\setcounter{enumi}{\thesauvegardeenumi}}

\title{Acceleration of GD via Momentum}
\date{\today}

\begin{document}
\maketitle
\frame{\tableofcontents}

\section{Optimal methods}

\begin{frame}
  \frametitle{Smooth convex functions: less than $\mathcal{O}(\epsilon^{-1})$ steps?}
  Given $L$ and $D=\Vert x_0 - x^* \Vert$ we know that
  \begin{itemize}
    \item GD converges with $\mathcal{O}(1/k)$
    \item cannot go faster (``lower bound'')
  \end{itemize}

  Maybe gradient descent is not the best possible algorithm? \medskip

  After all it is arguably the simplest possible method using the gradient.
\end{frame}


\begin{frame}
  \frametitle{Smooth convex functions: less than $\mathcal{O}(\epsilon^{-1})$ steps?}
  So let's look at the following classes of methods:

  First-order method:
  \begin{itemize}
    \item Access to the data only via an oracle which returns $f$ and $\nabla f$ at given points.
    \item Clearly GD is a first order method.
  \end{itemize}

  \textbf{Q:} What is the \textcolor{blue}{best} first-order method for smooth convex functions.\\
  \textcolor{gray}{\textit{best} means: smallest upper bound on the number of oracle calls \textit{in the worst case}.}

  \begin{itemize}
    \item Nemirovski and Yudin 1979 proved that every first-order method needs at least $\Omega(1/\sqrt{\epsilon})$ iterations (no method can be faster than $\mathcal{O}(1/k^2)$).
  \end{itemize}

\end{frame}


\section{Nesterov momentum}%
\label{sec:}

\begin{frame}
  \frametitle{Acceleration to $\mathcal{O}(1/\sqrt{\epsilon})$ steps}

  \begin{itemize}
    \item Nesterov 1983 came up with a method that needs only $\mathcal{O}(1/\sqrt{\epsilon})$ iterations (and is therefore the \textit{best one}).
    \item Known as \textcolor{blue}{\textbf{Nesterov's accelerated gradient}} method.
    \item By now multiple similar algorithms with same complexity exist.
    \item Proofs are generally not really instructive (some are computer assisted).
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Nesterov's accelerated gradient method}
  \begin{algorithm}[H]
    \caption{Nesterov's accelerated gradient method (NAG)}
    \begin{algorithmic}[1]
      \For{$ k = 0, 1, \dots$}
      \State{$x_{k+1} = y_k - \frac{1}{L} \nabla f(y_k)$}
      \State{$z_{k+1} = z_k - \frac{k+1}{2L} \nabla f(y_k)$}
      \State{$y_{k+1} = \frac{k+1}{k+3} x_{k+1} + \frac{2}{k+3} z_{k+1}$}
      \EndFor{}
    \end{algorithmic}
  \end{algorithm}

  % \begin{align}
  %   x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k) \\
  %   z_{k+1} &= z_k - \frac{t+1}{2L} \nabla f(y_k) \\
  %   y_{k+1} &= \frac{t+1}{t+3} x_{k+1} + \frac{2}{t+3} z_{k+1}
  % \end{align}

  \begin{itemize}
    \item perform ``\textbf{smooth step}'' from $y_k$ to $x_{k+1}$
    \item perform \textbf{aggressive step} from $z_k$ to $z_{k+1}$
    \item form \textbf{weighted average} of the two\\
          compensate for the aggressive step by giving less weight
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Nesterov's algorithm as a momentum method}
  A different way to write the method is via \textcolor{blue}{momentum}
  \begin{align}
    y_{k} &= x_k + \beta_k (x_k - x_{k-1}) \\
    x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k).
  \end{align}

  \begin{itemize}
    \item differs from GD on in momentum/inertia term $\beta_k (x_k - x_{k-1})$
    \item has to chosen carefully $\beta_k = \frac{k-1}{k+2}$
    \item coefficient approaches $\frac{k-1}{k+2} \approx 1 - \frac{3}{k}$
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Nesterov's accelerated gradient method: convergence}

  \begin{theorem}
    Let $f: R^d \to \R$ be convex and $L$-smooth with minimum $x^*$, then NAG yields
    \begin{equation}
      f(x_k) - f(x^*) \le \frac{2L \Vert x_0-x^* \Vert^2}{k(k+1)}
    \end{equation}
  \end{theorem}

  Recall that the gradient descent bound was
  \begin{equation}
      f(x_k) - f(x^*) \le \frac{L \Vert x_0-x^* \Vert^2}{2k}.
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{Proof idea}
  Potential function $\Phi$ that decreases along trajectory (standard technique).
  Out of the blue: Use
  \begin{equation}
    \Phi(k) := k(k+1) (f(x_k) - f^*) + 2L \Vert z_k - x^* \Vert^2.
  \end{equation}
  Then show that
  \begin{equation}
    \Phi(k+1) \le \Phi(k).
  \end{equation}
  Results in
  \begin{equation}
    \Phi(k+1) \le \Phi(k) \le \cdots \le \Phi(0)
  \end{equation}
  and therefore
  \begin{equation}
    k(k+1) (f(x_k) - f^*)  \le 2L \Vert z_0 - x^* \Vert^2.
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{Why momentum?}
  \begin{itemize}
    \item GD has problems with \textbf{ravines}, i.e.\ areas where the surface curves much more steeply in one dimension than in another.
    \item Results in zig-zagging.
  \end{itemize}

  \begin{minipage}{0.48\textwidth}
    \begin{figure}[ht]
      \centering
      \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{gd_zig_zag}
      \caption{no momentum}
    \end{figure}
  \end{minipage}
  \begin{minipage}{0.48\textwidth}
    \begin{figure}[ht]
      \centering
      \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{gd_zig_zag_momentum}
      \caption{with momentum}
    \end{figure}
  \end{minipage}
\end{frame}

\begin{frame}
  \frametitle{Momentum in terms of velocity}
  Consider a ball rolling down a slope. Its \textbf{velocity} is
  \begin{align}
    v_k &= \beta v_{k-1} + \alpha \nabla f(x_k) \\
    x_{k+1} &= x_k - v_k
  \end{align}
  \begin{itemize}
    \item a fraction $\beta$ of the \textbf{previous velocity} (friction)
    \item plus, steepness of the \textbf{slope}
  \end{itemize}
  In terms of iterates:
  \begin{align}
    x_{k+1} &= x_k - v_k \\
            &= x_k - \alpha \nabla f(x_k) - \beta v_{k-1} \\
            &= x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
  \end{align}
\end{frame}


\section{Heavy ball}%
\label{sec:}

\begin{frame}
  \frametitle{Heavy ball:  Polyak 1964}
  We derived
  \begin{equation}
    x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1}),
  \end{equation}
  while Nesterov's method was
  \begin{align}
    y_{k} &= x_k + \beta_k (x_k - x_{k-1}) \\
    x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k).
  \end{align}
  However, \textcolor{blue}{\textbf{Polyak's}} momentum provides no speedup over $\mathcal{O}(1/k)$ \textcolor{gray}{for smooth convex function}.
\end{frame}


\begin{frame}
  \frametitle{What's the difference?}
  \begin{itemize}
    \item Both types of momentum seem so similar.
    \item Heavy ball does not care if do momentum or gradient first.
    \item Nesterov momentum applies inertia first then gradient.
          \begin{align}
            v_k &= \beta v_{k-1} + \alpha \nabla f(x_k + \beta v_{k-1}) \\
            x_{k+1} &= x_k - v_k
          \end{align}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}

  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth,keepaspectratio]{nesterov-vs-polyak.png}
    \caption{Nesterov vs Polyak momentum.}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Momentum for strongly convex functions}

  For smooth strongly convex we know that GD obtains
  \begin{equation}
    \Vert x_{k+1} - x^* \Vert^2 \le (1 - \frac{\mu}{L}) \Vert x_k -x^* \Vert^2
  \end{equation}
  and
  \begin{equation}
    f(x_k)-f^* \le {\left(1-\frac{\mu}{L}\right)}^k \, \frac{L \Vert x_0 -x^* \Vert^2}{2}.
  \end{equation}
  Performance depends heavily on the \textbf{condition number} $\kappa : = L/\mu$:
  \begin{block}{}
    Contraction coefficient is $(1-1/\kappa)$.
  \end{block}

  Nesterov \textbf{and Polyak} momentum improve this to $(1-1/\sqrt{\kappa})$
\end{frame}


\begin{frame}
  \frametitle{Momentum for stochastic methods}
  SGD analysis can be extended to \textbf{smooth} functions with rate
  \begin{equation}
    \mathcal{O}\left(\frac{L}{k} + \frac{\sigma^2}{\sqrt{k}}\right)
  \end{equation}
  where $\sigma^2:= \E [ \Vert \nabla f(x) - g(x) \Vert^2 ]$ is the \textbf{variance} of the gradient estimator.

  This can be improved by momentum (and additional tricks) to
  \begin{equation}
    \mathcal{O}\left(\frac{L}{k^2} + \frac{\sigma^2}{\sqrt{k}}\right).
  \end{equation}

  Improvement only in the ``\textbf{transient phase}'' before noise takes over.

  For worst case rates, only the asymptotic phase matters.

\end{frame}

\begin{frame}
  \frametitle{Momentum in the nonconvex world}

  While it is extremely difficult to show a benefit of momentum in for nonconvex problems.

  Empirical evidence (for many different problems) is strong.

  Theory is mostly limited to escaping of saddle points.

\end{frame}


\end{document}
