\documentclass{scrartcl}

\input{../../shared.tex}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\usepackage{enumerate}

\begin{document}


\section*{Problem set: Gradient descent}%


\paragraph{Exercise (i)} (7P) Enter the missing code snippets in the jupyter notebook. Partial credit will be awarded.


\begin{definition}
  For a matrix $Q \in \R^{m \times d}$ its \textbf{operator norm} is defined as
  \begin{equation}
    \Vert Q \Vert_{op} := \sup_{x \in \R^d: \Vert x \Vert \le 1} \Vert Q x \Vert,
  \end{equation}
  but for convenience we will mostly write $\Vert Q \Vert$ (so no subscript) but mean the operator norm.
\end{definition}

\begin{lemma}%
  The operator norm of a matrix $Q$ fulfills
  \begin{equation}
    \Vert Qx \Vert \le \Vert Q \Vert_{op} \Vert x \Vert.
  \end{equation}
  In particular, every linear map (given by a matrix $Q$) is Lipschitz continuous with Lipschitz constant $\Vert Q \Vert$.
\end{lemma}


\paragraph{Exercise (ii)} (2P) Prove that the quadratic function
\begin{equation}
  f(x) = \frac{1}{2} x^T Q x +b^T x + c
\end{equation}
is \textbf{smooth} with parameter $\Vert Q \Vert$.


\paragraph{Exercise (iii)} (1P) Suppose that we have observations $(x_i, y_i)$ which are \textbf{centered}, meaning that $\sum_{i=1}^{n}x_i = 0 = \sum_{i=1}^{n}y_i$. Let $(b^*, w^*)$ be the global minimum of the least squares objective
\begin{equation}
  f(b, w) = \sum_{i=1}^{n} {(b + w^T x_i - y_i)}^2.
\end{equation}
Prove that $b^*=0$.



\end{document}
