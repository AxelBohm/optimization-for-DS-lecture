{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22ecccc",
   "metadata": {
    "tags": []
   },
   "source": [
    "$\\newcommand{\\n}[1]{\\|#1 \\|}$\n",
    "$\\renewcommand{\\a}{\\alpha}$\n",
    "$\\renewcommand{\\b}{\\beta}$\n",
    "$\\renewcommand{\\c}{\\gamma}$\n",
    "$\\renewcommand{\\d}{\\delta}$\n",
    "$\\newcommand{\\la}{\\lambda}$\n",
    "$\\newcommand{\\e}{\\varepsilon}$\n",
    "$\\renewcommand{\\t}{\\tau}$\n",
    "$\\renewcommand{\\th}{\\theta}$\n",
    "$\\newcommand{\\s}{\\sigma}$\n",
    "$\\newcommand{\\x}{\\bar x}$\n",
    "$\\newcommand{\\n}[1]{\\left\\|#1 \\right\\|}$ \n",
    "$\\newcommand{\\R}{\\mathbb R}            $ \n",
    "$\\newcommand{\\N}{\\mathbb N}            $ \n",
    "$\\newcommand{\\Z}{\\mathbb Z}            $ \n",
    "$\\newcommand{\\lr}[1]{\\left\\langle #1\\right\\rangle}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "\n",
    "## **Robust regression --- Mirror descent** \n",
    "\n",
    "If you have any questions you can additionally look at [this link](https://web.stanford.edu/class/cs229t/2017/Lectures/mirror-descent.pdf).\n",
    "\n",
    "*Regression*: we have a sequence of vectors $a_i\\in \\R^n$ and measurements $b_i\\in \\R$ for $i=1,\\dots, m$. \n",
    "We want to predict $b_i$ using $\\lr{a_i, x}$ for some unknown $x\\in \\R^n$. \n",
    "\n",
    "Because of possible outliers (or other reasons) $\\ell_2$-regression may be not a good choice, we want a more robust approach. \n",
    "\n",
    "Given $A\\in \\R^{m\\times n}$ and $b\\in \\R^m$, minimize\n",
    "$$ f(x) = \\frac{1}{m} \\n{Ax-b}_1  = \\frac{1}{m}\\sum_{i=1}^m |\\lr{a_i,x} - b_i|$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3d99bd",
   "metadata": {},
   "source": [
    "First, answer the following questions:\n",
    "\n",
    " - Is the problem convex?\n",
    " - Is the problem smooth / constrained?\n",
    " - What are the dimensions?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108bbaf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "#### **Computing the subgradient**\n",
    " What is the subgradient of $f$ at $x$? For $y\\mapsto \\n{y}_1$ it is $\\sign(y)$ (at zero any value from $[-1,1]$ will work). Thus,\n",
    " $$g(x) = \\frac{1}{m} A^\\top \\sign(Ax-b) = \\frac{1}{m} \\sum_{i=1}^m a_i \\sign(\\lr{a_i,x} -b_i)$$\n",
    " \n",
    "#### **Computing the stochastic subgradient**\n",
    " What is the stochastic subgradient of $f$ at $x$? One option is to take\n",
    " $$g_i(x) = a_i \\sign(\\lr{a_i, x} -b_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b807bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import datetime\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86cf699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(A, b, x):\n",
    "    raise NotImplementedError\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e0828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient(A, b, x):\n",
    "    return 1/b.shape[0] * A.T * np.sign(A @ x - b)\n",
    "\n",
    "def stochastic_subgradient(A, b, x, i):\n",
    "    return A[i] @ np.sign(A[i] @ x - b[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecfc680",
   "metadata": {},
   "source": [
    "In the lecture we have seen that when we pick the negative entropy $x\\log(x)$ as distance generating function for the Bregman distance, then the mirror descent update becomes\n",
    "$$\n",
    "x_{k+1} = x_k e^{-\\alpha \\nabla f(x_k)}\n",
    "$$\n",
    "and is therefore sometimes referred to as *exponential gradient*, or *entropic mirror descent* or *entropic descent*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877db9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropic_md_update(x, lr, grad):\n",
    "    \"\"\"a.k.a. Mirror descent with respect the Bregman distance generated by the negative entropy\"\"\"\n",
    "    # todo implement the above update\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d17b2",
   "metadata": {},
   "source": [
    "To enforce the unit simplex constraint the corresponding **Bregman projection** $\\Pi_h$ (a concept we haven't discussed but should be easy to understand) is given simply by normalizing the input (w.r.t. the $1$-norm), i.e.\n",
    "$$\n",
    "  \\Pi_h(x) = \\frac{x}{\\lVert x \\rVert_1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bregman_projection_onto_simplex(x):\n",
    "    # todo implement the above update\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4b8a5",
   "metadata": {},
   "source": [
    "Sometimes the nomenclature is not consistent and only the method that does both - the mirror descent step, and the Bregman projection - is called *entropic mirror descent*. So for clarity we call the next function `full_entropic_md_update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28561ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_entropic_md_update(x, lr, grad):\n",
    "    \"\"\"Compute the update of one iteration of \"\"\"\n",
    "    y = entropic_md_update(x, lr, grad)\n",
    "    x = \n",
    "    raise NotImplementedError\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03174781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror_descent(A, b, x_0, max_iters, stoch=True):\n",
    "    \"\"\"Run (stochastic) mirror descent method\"\"\"\n",
    "    xs = [x_0]  # parameters after each update \n",
    "    objectives = []  # loss values after each update\n",
    "    x = x_0\n",
    "    for iteration in range(max_iters):\n",
    "        if stoch == True:\n",
    "            i = randint(len(b))\n",
    "            grad = \n",
    "        else:\n",
    "            grad =\n",
    "        \n",
    "        x = full_entropic_md_update(x, lr, grad)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement (stochastic) mirror descent\n",
    "        # ***************************************************   \n",
    "        raise NotImplementedError\n",
    "\n",
    "        if iteration % 1000 == 0:\n",
    "            print(\"MD({bi:04d}/{ti:04d}): objective = {l:10.2f}\".format(\n",
    "                  bi=iteration, ti=max_iters - 1, l=objective))\n",
    "    return objectives, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06704af3",
   "metadata": {},
   "source": [
    "For the projected subgradient method you will need the following function which gives the projection onto the unit simplex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_simplex(v, z=1):\n",
    "    \"\"\"Projection onto unit simplex.\"\"\"\n",
    "    n_features = v.shape[0]\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u) - z\n",
    "    ind = np.arange(n_features) + 1\n",
    "    cond = u - cssv / ind > 0\n",
    "    rho = ind[cond][-1]\n",
    "    theta = cssv[cond][-1] / float(rho)\n",
    "    w = np.maximum(v - theta, 0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_sgd(A, b, x_0, max_iters, stoch=True):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent with projectoin onto unit simplex\n",
    "    \n",
    "    :param b: numpy array of size (num_examples)\n",
    "    :param A: numpy array of size (num_examples, num_features)\n",
    "    :param x_0: starting parameters, a numpy array of size (num_features)\n",
    "    :param max_iters: integer, number of updates to do\n",
    "    \n",
    "    :return:\n",
    "    - objectives, a list of loss values\n",
    "    - param_states, a list of parameter vectors,\n",
    "    \"\"\"\n",
    "    xs = [x_0]  # parameters after each update \n",
    "    obj = []  # loss values after each update\n",
    "    x = x_0\n",
    "    for iteration in range(max_iters):\n",
    "        if stoch == True:\n",
    "            i = randint(len(b))\n",
    "            grad = \n",
    "        else:\n",
    "            grad =\n",
    "        \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement (stochastic) gradient descent + projection\n",
    "        # ***************************************************   \n",
    "        raise NotImplementedError\n",
    "        \n",
    "        # store x and objective\n",
    "        xs.append(x.copy())\n",
    "        obj = objective(A, b, x)\n",
    "        obj.append(obj)\n",
    "\n",
    "        if iteration % 1000 == 0:\n",
    "            print(\"SGD({bi:04d}/{ti:04d}): objective = {l:10.2f}\".format(\n",
    "                  bi=iteration, ti=max_iters - 1, l=objective))\n",
    "    return obj, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca9484",
   "metadata": {},
   "source": [
    "## Problem setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef0c88",
   "metadata": {},
   "source": [
    "Compare the version of mirror descent given by the update in `full_entropic_md_update` to the projected sub-gradient method. Both for **stochastic vs deterministic** and **last iterate** vs **averaged iterates**. On the following problem:\n",
    "\n",
    "$$\n",
    "\\min_{x\\in \\Delta^d} f(x) = \\frac{1}{m} \\n{Ax-b}_1\n",
    "$$\n",
    "where $\\Delta^d = \\{ x \\in \\R^d: x\\ge 0, \\n{x}_1 \\le 1\\}$ is the unit simplex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3758f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import t \n",
    "A, _, xstar = make_regression(n_samples=10000, n_features=1000, coef=True)\n",
    "xstar = np.abs(xstar)/ np.linalg.norm(xstar, 1)  # make solution element of unit simplex\n",
    "noise = t.rvs(df=1, size=A.shape[0])    # one degree of freedom for fat tails\n",
    "b = A @ xstar + 0.1*noise\n",
    "best_objective = objective(A, b, xstar)\n",
    "print('Number of samples n = ', b.shape[0])\n",
    "print('Dimension of each sample d = ', A.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = int(1e4)\n",
    "alpha = 0.2   # this is totally arbitrary, try different learning rates\n",
    "\n",
    "# Initialization\n",
    "x_0 = np.ones(A.shape[1])/ A.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b9670",
   "metadata": {},
   "source": [
    "## Run numerical experiments\n",
    "\n",
    "- Compare subgradient method and mirror descent as well as stochastic subgradient method and stochastic mirror descent,\n",
    "\n",
    "- Play also with the problem size and try and find a setting where mirror descent outperforms the standard subgradient method.\n",
    "\n",
    "- Experiment with the dimension of the problem and compare the 2-norm and $\\infty$-norm of the gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541620f7",
   "metadata": {},
   "source": [
    "run projected SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d42d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_objectives, sgd_xs = projected_sgd(A, b, x_0, max_iters, alpha)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1dd0f",
   "metadata": {},
   "source": [
    "run mirror descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e641b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start stochastic mirror descent.\n",
    "start_time = datetime.datetime.now()\n",
    "md_objectives, sgd_xs = mirror_descent(A, b, x_0, max_iters, alpha)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"MD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf397b14",
   "metadata": {},
   "source": [
    "plot convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Convergence rates', fontsize = 20)\n",
    "plt.loglog(sgd_objectives - best_objective, label = r'$sgd \\alpha = {alpha}$')\n",
    "plt.loglog(md_objectives - best_objective, label = r'$md \\alpha = {alpha}$')\n",
    "plt.xlabel('iteration k'  , fontsize = 20)\n",
    "plt.ylabel(r'$f(x_k) - f(x^*)$', fontsize = 20)\n",
    "plt.legend(fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7a0aa",
   "metadata": {},
   "source": [
    "more experiments here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd88ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b0f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809d87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028905f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
