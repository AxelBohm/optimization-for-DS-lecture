{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in previous exercises, we will train a regression model to predict a person's weight from their height. The model is linear regression with one explanatory variable (weight) and an offset. The model is optimized under a least-squares loss. The variable $b$ contains the prediction targets (a vector of the length of the dataset) and the variable $A$ is the data matrix, containing (1) a column of ones and (2) a column with the explanatory variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "b, A = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n, d) =  (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "print('(n, d) = ', A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Estimation\n",
    "Least squares estimation is one of the fundamental machine learning algorithms. Given an $ n \\times d $ matrix $A$ and a $ n \\times 1$ vector $b$, the goal is to find a vector $x \\in \\mathbb{R}^d$ which minimizes the objective function $$f(x) = \\frac{1}{2n} \\sum_{i=1}^{n} (a_i^\\top x - b_i)^2 = \\frac{1}{2n} \\|Ax - b\\|^2 $$\n",
    "\n",
    "In this exercise, we will try to fit $x$ using Least Squares Estimation. \n",
    "\n",
    "One can see the function is $\\mu$ strongly convex with $\\mu = \\lambda_{min}(\\nabla^2 f(x)) = \\frac{1}{n} \\lambda_{min}(A^T A) $ and $L$ smooth with $L = \\lambda_{max}(\\nabla^2 f(x)) = \\frac{1}{n} \\lambda_{max}(A^T A)$, since here the Hessian matrix is constant, independent of $x$.\n",
    "\n",
    "When $A^{T} A$ is invertible, which is the case here, then we have a simple closed form solution for (the unique) $x^* = \\text{argmin}_{x \\in \\mathbb{R}^d} f(x)$. Indeed, solving $\\nabla f(x^*) = 0$ leads to $x^* = (A^T A)^{-1} A^T b$. However when $n$ and $d$ are big (which is the case in modern \"big data\" problems), using this formula is prohibitively expensive from a computational viewpoint, hence the necessity of using algorithms such as SGD, which consider a datapoint after the other.\n",
    "\n",
    "In this lab, since $n$ and $d$ are small, and in order to exhibit the convergence rates seen in class, we start by computing $f(x^*)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_objective(targets_b, data_A, params_x):\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's same as the gradient descent.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = np.linalg.solve(A.T@A, A.T@b)\n",
    "best_objective = full_objective(b, A, x_star)\n",
    "print(\"f(x*) = \", best_objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the function `minibatch_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient(targets_b, data_A, params_x):\n",
    "    \"\"\"\n",
    "    Compute a mini-batch stochastic gradient from a subset of `num_examples` from the dataset.\n",
    "    \n",
    "    :param targets_b: a numpy array of shape (num_examples)\n",
    "    :param data_A: a numpy array of shape (num_examples, num_features)\n",
    "    :param params_x: compute the mini-batch gradient at these parameters, numpy array of shape (num_features)\n",
    "    \n",
    "    :return: gradient: numpy array of shape (num_features)\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's same as the gradient descent.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify empirically for a fixed parameter vector $x$ that the expected value of your `minibatch_gradient` function equals the full gradient. Validating this property for a mini-batch of size 1 is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10\n",
    "for i in range(num_trials):\n",
    "    # Try different parameter vectors $x$\n",
    "    x = np.random.rand(A.shape[1])\n",
    "\n",
    "    # Compute the mean of all stochastic gradient for mini-batch size 1\n",
    "    stochastic_gradients = []\n",
    "    for i in range(len(b)):\n",
    "        stochastic_gradients.append(minibatch_gradient(b[i:i+1], A[i:i+1, :], x))\n",
    "\n",
    "    # Compute the full gradient\n",
    "    full_gradient = minibatch_gradient(b, A, x)\n",
    "\n",
    "    # Those should be the same on average\n",
    "    assert np.allclose(np.mean(stochastic_gradients, axis=0), full_gradient)\n",
    "    \n",
    "print('Tests passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement stochastic gradient descent for Linear Least Squares, below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient(targets_b, data_A, params_x, batch_size=1):\n",
    "    \"\"\"\n",
    "    Compute a stochastic gradient\n",
    "    \n",
    "    :param targets_b: numpy array of size (num_examples)\n",
    "    :param data_A: numpy array of size (num_examples, num_features)\n",
    "    :param params_x: compute the mini-batch gradient at these parameters, numpy array of shape (num_features)\n",
    "    :param batch_size: integer: number of datapoints to compute the stochastic gradient from\n",
    "    \n",
    "    :return: gradient, numpy array of shape (num_features)\n",
    "    \"\"\"\n",
    "    dataset_size = len(targets_b)\n",
    "    indices = np.random.choice(dataset_size, batch_size, replace=False)\n",
    "    return minibatch_gradient(targets_b[indices], data_A[indices, :], params_x)\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        targets_b, \n",
    "        data_A, \n",
    "        initial_x, \n",
    "        batch_size, \n",
    "        max_iters, \n",
    "        initial_learning_rate, \n",
    "        decreasing_learning_rate=False):\n",
    "    \"\"\"\n",
    "    Mini-batch Stochastic Gradient Descent for Linear Least Squares problems.\n",
    "    \n",
    "    :param targets_b: numpy array of size (num_examples)\n",
    "    :param data_A: numpy array of size (num_examples, num_features)\n",
    "    :param initial_x: starting parameters, a numpy array of size (num_features)\n",
    "    :param batch_size: size of the mini-batches\n",
    "    :param max_iters: integer, number of updates to do\n",
    "    :param initial_learning_rate: float\n",
    "    :param decreasing_learning_rate: if set to true, the learning rate should decay as 1 / t \n",
    "    \n",
    "    :return:\n",
    "    - objectives, a list of loss values on the whole dataset, collected at the end of each pass over the dataset (epoch)\n",
    "    - param_states, a list of parameter vectors, collected at the end of each pass over the dataset\n",
    "    \"\"\"\n",
    "    xs = [initial_x]  # parameters after each update \n",
    "    objectives = []  # loss values after each update\n",
    "    x = initial_x\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        grad = stochastic_gradient(targets_b, data_A, x, batch_size=batch_size)\n",
    "        if decreasing_learning_rate:\n",
    "            lr = initial_learning_rate / (iteration + 1)\n",
    "        else:\n",
    "            lr = initial_learning_rate\n",
    "        \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "    # ***************************************************   \n",
    "    raise NotImplementedError\n",
    "\n",
    "        if iteration % 1000 == 0:\n",
    "            print(\"SGD({bi:04d}/{ti:04d}): objective = {l:10.2f}\".format(\n",
    "                  bi=iteration, ti=max_iters - 1, l=objective))\n",
    "    return objectives, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to test your optimizer with a naive step size with the example code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = int(1e4)\n",
    "gamma = 0.2   # this is totally arbitrary\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_objectives, sgd_xs = stochastic_gradient_descent(\n",
    "    b, A, x_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Convergence rates', fontsize = 20)\n",
    "plt.loglog(sgd_objectives - best_objective, label = r'$\\gamma = 0.2$')\n",
    "plt.xlabel('iteration t'  , fontsize = 20)\n",
    "plt.ylabel(r'$f(x_t) - f(x^*)$', fontsize = 20)\n",
    "plt.legend(fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that using an arbitrary constant step size does not lead to appropriate convergence: the iterates \"saturate\" because of the stochastic noise and stop making progress at some point. We thus need to use more appropriate step sizes as given Theorem 5.1 and 5.2 from the lecture notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assuming bounded expected stochastic gradients\n",
    "Assume we are moving in a bounded region $\\|x\\| \\leq 25$ containing all iterates (and we assume $\\|x-x^\\star\\| \\leq 25$ as well, for simplicity). By $\\nabla f(x) = \\frac{1}{n}A^\\top (Ax - b)$, one can see that $f$ is Lipschitz over that bounded region, with Lipschitz constant $\\|\\nabla f(x)\\| \\leq \\frac{1}{n} (\\|A^\\top A\\|\\|x\\| + \\|A^\\top b\\|)$. We also know that $E\\big[\\|g_t\\|\\big | x_t\\big]\\ = \\nabla f(x)$. So to find B such that  $E\\big[\\|g_t\\|^2\\big]\\leq B^2$, we need to compute the Lipschitz constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = int(1e4)  # 10 passes through the dataset\n",
    "\n",
    "gamma =  # Fill in a better learning rate  \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_objectives_dec_gamma, sgd_xs_dec_gamma = stochastic_gradient_descent(\n",
    "    b, A, x_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time visualization with a better learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Convergence rates', fontsize = 20)\n",
    "plt.loglog(sgd_objectives - best_objective, label = r'$\\gamma = 0.2$')\n",
    "plt.loglog(sgd_objectives_dec_gamma - best_objective, label = r'$\\gamma = R B / \\sqrt{T}$')\n",
    "plt.xlabel('iteration t'  , fontsize = 20)\n",
    "plt.ylabel(r'$f(x_t) - f(x^*)$', fontsize = 20)\n",
    "plt.legend(fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD using strong convexity\n",
    "\n",
    "We didn't discuss this in the lecture, but the similarly to how regular GD performs better in the presence of strong convexity the same holds true for subgradient method and SGD.\n",
    "For optimal performance the stepsize now has to decrease across iterations and is proportional to the strong convexity constant. (Both things have been already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = int(1e4)\n",
    "\n",
    "# You may use these results from last week\n",
    "mu = np.linalg.norm(A.T@A, -2) / len(A)   \n",
    "L = np.linalg.norm(A.T@A, 2) / len(A)\n",
    "\n",
    "gamma0 = 2 / mu\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_objectives_dec_gamma_mu, sgd_xs_dec_gamma_mu = stochastic_gradient_descent(\n",
    "    b, A, x_initial, batch_size, max_iters, gamma0, decreasing_learning_rate=True)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the convergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Convergence rates', fontsize = 20)\n",
    "plt.loglog(sgd_objectives - best_objective, label = r'$\\gamma = 0.2$')\n",
    "plt.loglog(sgd_objectives_dec_gamma - best_objective, label = r'$\\gamma = R B / \\sqrt{T}$')\n",
    "plt.loglog(sgd_objectives_dec_gamma_mu - best_objective, label = r'$\\gamma = 1 / \\mu (t + 1)$')\n",
    "plt.xlabel('iteration t'  , fontsize = 20)\n",
    "plt.ylabel(r'$f(x_t) - f(x^*)$', fontsize = 20)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the convergence rate for $\\gamma_t \\sim \\frac{1}{t} $ is indeed $O(\\frac{1}{t})$ as stated Theorem 5.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = np.arange(1, max_iters)\n",
    "\n",
    "plt.title('Convergence rates', fontsize = 20)\n",
    "plt.loglog(sgd_objectives_dec_gamma_mu - best_objective, label = r'$\\gamma = 1 / \\mu (t + 1)$', color = 'g')\n",
    "plt.loglog(20 / tab, label = r'$O(1 / t)$ rate', color = 'k', linestyle = 'dotted')\n",
    "plt.xlabel('iteration t'  , fontsize = 20)\n",
    "plt.ylabel(r'$f(x_t) - f(x^*)$', fontsize = 20)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last plot we clearly see the advantage of using log log plots. Indeed otherwise it is impossible to deduce anything: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = np.arange(1, max_iters)\n",
    "\n",
    "plt.title('Convergence rates', fontsize = 20)\n",
    "plt.plot(sgd_objectives_dec_gamma_mu - best_objective, label = r'$\\gamma = 1 / \\mu (t + 1)$')\n",
    "plt.plot(20 / tab, label = r'$O(1 / t)$ rate', color = 'k', linestyle = 'dotted')\n",
    "plt.xlabel('iteration t'  , fontsize = 20)\n",
    "plt.ylabel(r'$f(x_t) - f(x^*)$', fontsize = 20)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to copy your code for the gradient descent implementation from lab03 into the file gradient_descent.py\n",
    "\n",
    "Run the following code to compare SGD (with diffent stepsizes) and gradient descent with respect to the number of gradient computations needed per iteration (remember that gradient descent computes n gradients per iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import gradient_descent\n",
    "T = 5 # number of full gradient computed\n",
    "\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "gradient_objectives, gradient_xs = gradient_descent(b, A, x_initial, T, 1/(2 *L))\n",
    "# Define the parameters of the algorithm.\n",
    "\n",
    "max_iters = T * A.shape[0] # number of stochastic gradients which can be computed during the same time T (for batch size 1)\n",
    "sgd_objectives_dec_gamma_mu, sgd_xs_dec_gamma = stochastic_gradient_descent(\n",
    "    b, A, x_initial, 1, max_iters, 2 / mu, decreasing_learning_rate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that SGD takes a pretty long time to run because we ask it to compute the full objective at each iteration, this is not done in practice because it is computationally heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Convergence rates', fontsize = 20)\n",
    "plt.loglog(range(0,len(gradient_objectives)*10000,10000),\n",
    "         gradient_objectives - best_objective,'g', label='gradient descent with 1/2L stepsize')\n",
    "plt.loglog(sgd_objectives_dec_gamma_mu - best_objective, label = r'$\\gamma_t = 1 / \\mu (t + 1)$')\n",
    "plt.xlabel(r'number of $\\nabla f_i$ computed'  , fontsize = 20)\n",
    "plt.ylabel(r'$f(x_t) - f(x^*)$', fontsize = 20)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that SGD is much faster than GD at the beginning (GD will eventually become faster) and therefore makes it computationally more attractive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading more complex data\n",
    "The data is taken from https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"Concrete_data.csv\",delimiter=\",\")\n",
    "\n",
    "A = data[:,:-1]\n",
    "b = data[:,-1]\n",
    "A, mean_A, std_A = standardize(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape, A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = np.linalg.solve(A.T@A, A.T@b)\n",
    "best_objective = full_objective(b, A, x_star)\n",
    "print('f(x*) = ', best_objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your SGD algorithm on the new dataset. Try different learning rates. Is the outcome very stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iters = int(1e4)\n",
    "\n",
    "gamma =  0.01  # totally arbitrary      \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_objectives, sgd_xs = stochastic_gradient_descent(\n",
    "    b, A, x_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Evolution of the Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Convergence rates', fontsize = 20)\n",
    "plt.loglog(sgd_objectives - best_objective, label = r'$\\gamma = 0.001$')\n",
    "plt.xlabel(r'number of $\\nabla f_i$ computed'  , fontsize = 20)\n",
    "plt.ylabel(r'$f(x_t) - f(x^*)$', fontsize = 20)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projected Stochastic Gradient Descant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid $x$ becoming too big, we can perform constrained optimization by projecting x onto an $\\text{L}_2$ ball at each iteration, thus limiting the norm of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `projection` function below, which projects x onto an L2-ball:\n",
    "(make sure the optimum is inside the l2-ball by choosing an appropriate radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_ball_radius =   # choose an appropriate radius\n",
    "def projection(x):\n",
    "    \"\"\"project x onto an l2-ball\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute the projection of x onto the l2-ball\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `projected_stochastic_gradient_descent` function below:<br>\n",
    "(Hint: it is the same as stochastic_gradient_descent but with an extra step in the loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_stochastic_gradient_descent(\n",
    "        b, \n",
    "        A, \n",
    "        initial_x, \n",
    "        batch_size, \n",
    "        max_iters, \n",
    "        initial_learning_rate,\n",
    "        decreasing_learning_rate = False, \n",
    "        projection_fn = lambda x: x):\n",
    "\n",
    "    \"\"\"Projected gradient descent.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    return objectives, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your projected SGD function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = int(1e4)\n",
    "\n",
    "gamma =  # Fill in a learning rate      \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "psgd_objectives, psgd_xs = projected_stochastic_gradient_descent(\n",
    "    b, A, x_initial, batch_size, max_iters, gamma,\n",
    "    projection_fn=projection)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Convergence rates', fontsize = 20)\n",
    "plt.loglog(sgd_objectives - best_objective, label = r'$\\gamma = 0.001$')\n",
    "plt.loglog(psgd_objectives - best_objective,'r', label='projected sgd')\n",
    "plt.xlabel(r'number of $\\nabla f_i$ computed'  , fontsize = 20)\n",
    "plt.ylabel(r'$f(x_t) - f(x^*)$', fontsize = 20)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
