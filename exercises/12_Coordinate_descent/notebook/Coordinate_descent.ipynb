{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse as sps\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Support Vector Machines\n",
    "## Classification Using SVM\n",
    "Load dataset. We will use w1a dataset from LibSVM datasets https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original optimization problem for the Support Vector Machine (SVM) is given by\n",
    "\\begin{equation}\\label{eq:primal}\n",
    "  \\min_{w \\in R^d} \\  \\sum_{i=1}^n \\ell(y_i A_i^\\top w) + \\frac\\lambda2 \\|w\\|^2\n",
    "\\end{equation}\n",
    "where $\\ell : R\\rightarrow R$, $\\ell(z) := \\max\\{0,1-z\\}$ is the hinge loss function.\n",
    "Here for any $i$, $1\\le i\\le n$, the vector $A_i\\in R^d$ is the $i$-th data example, and $y_i\\in\\{\\pm1\\}$ is the corresponding label.\n",
    "  \n",
    "The dual optimization problem for the SVM is given by \n",
    "\\begin{equation}\\label{eq:dual}\n",
    " \\max_{\\boldsymbol{\\alpha} \\in R^n } \\  \\alpha^\\top\\boldsymbol{1} - \\tfrac1{2\\lambda} \\alpha^\\top Y A A^\\top Y\\alpha\n",
    " \\text{    such that    $0\\le \\alpha_i \\le 1  \\ \\forall i$}\n",
    "\\end{equation}\n",
    "where $Y := \\mathop{diag}(y)$, and $A\\in R^{n \\times d}$ again collects all $n$ data examples as its columns. \n",
    "\n",
    "Note that $w$ can be derived from $\\alpha$ as\n",
    "\\begin{equation}\n",
    "    w(\\alpha) = \\frac{1}{\\lambda} A^\\top Y \\alpha.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/w1a'\n",
    "\n",
    "A, y = load_svmlight_file(DATA_TRAIN_PATH)\n",
    "A = A.toarray()\n",
    "print(y.shape, A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare cost and prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_primal_objective(y, A, w, lambda_):\n",
    "    \"\"\"\n",
    "    Compute the full cost (the primal objective), that is loss plus regularizer.\n",
    "    y: +1 or -1 labels, shape = (num_examples)\n",
    "    A: Dataset matrix, shape = (num_examples, num_features)\n",
    "    w: Model weights, shape = (num_features)\n",
    "    return: scalar value\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y, A, w):\n",
    "    \"\"\"\n",
    "    Compute the training accuracy on the training set (can be called for test set as well).\n",
    "    y: +1 or -1 labels, shape = (num_examples)\n",
    "    A: Dataset matrix, shape = (num_examples, num_features)\n",
    "    w: Model weights, shape = (num_features)\n",
    "    return: scalar value\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Descent (Ascent) for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the closed-form update for the i-th variable alpha, in the dual optimization problem, given alpha and the current corresponding w.\n",
    "\n",
    "\n",
    "Hints: \n",
    "- Differentiate the dual objective with respect to one `alpha[i]`.\n",
    "- Set the derivative to zero to compute a new `alpha[i]`.\n",
    "- Make sure the values of alpha stay inside a `[0, 1]` box.\n",
    "- You can formulate the update as `alpha[i] = projection(alpha[i] + lambda_ * (some update))`.\n",
    "- You can test the correctness of your implementation by checking if the difference between the dual objective and primal objective goes to zero. This difference, the duality gap, should get smaller than 10 in 700000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coordinate_update(y, A, lambda_, alpha, w, i):\n",
    "    \"\"\"\n",
    "    Compute a coordinate update (closed form) for coordinate i.\n",
    "    y: +1 or -1 labels, shape = (num_examples)\n",
    "    A: Dataset matrix, shape = (num_examples, num_features)\n",
    "    lambda_: Regularization parameter, scalar\n",
    "    alpha: Dual variables, shape = (num_examples)\n",
    "    w: Model weights, shape = (num_examples)\n",
    "    i: Index of the entry of the dual variable 'alpha' that is to be updated\n",
    "    return: New weights w (shape (num_features)), New dual variables alpha (shape (num_examples))\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    # calculate the update of coordinate at index=n.\n",
    "    a_i, y_i = A[i], y[i]\n",
    "    old_alpha_i = np.copy(alpha[i])\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    return w, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dual_objective(y, A, w, alpha, lambda_):\n",
    "    \"\"\"\n",
    "    Calculate the objective for the dual problem.\n",
    "    Follow the formula given above.\n",
    "    y: +1 or -1 labels, shape = (num_examples)\n",
    "    A: Dataset matrix, shape = (num_examples, num_features)\n",
    "    alpha: Dual variables, shape = (num_examples)\n",
    "    lambda_: Regularization parameter, scalar\n",
    "    return: Scalar value\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_descent_for_svm_demo(y, A, trace=False):\n",
    "    max_iter = 1000000\n",
    "    lambda_ = 0.01\n",
    "    history = defaultdict(list) if trace else None\n",
    "    \n",
    "    num_examples, num_features = A.shape\n",
    "    w = np.zeros(num_features)\n",
    "    alpha = np.zeros(num_examples)\n",
    "    \n",
    "    for it in range(max_iter):\n",
    "        # i = sample one data point uniformly at random from the columns of A\n",
    "        i = random.randint(0,num_examples-1)\n",
    "        \n",
    "        w, alpha = calculate_coordinate_update(y, A, lambda_, alpha, w, i)\n",
    "            \n",
    "        if it % 100000 == 0:\n",
    "            # primal objective\n",
    "            primal_value = calculate_primal_objective(y, A, w, lambda_)\n",
    "            # dual objective\n",
    "            dual_value = calculate_dual_objective(y, A, w, alpha, lambda_)\n",
    "            # primal dual gap\n",
    "            duality_gap = primal_value - dual_value\n",
    "            \n",
    "            print('iteration=%i, primal:%.5f, dual:%.5f, gap:%.5f'%(\n",
    "                    it, primal_value, dual_value, duality_gap))\n",
    "        if it % 1000 == 0:\n",
    "            primal_value = calculate_primal_objective(y, A, w, lambda_)\n",
    "            if trace:\n",
    "                history[\"objective_function\"] += [primal_value]\n",
    "                history['iter'].append(it)\n",
    "\n",
    "            \n",
    "    print(\"training accuracy = {l}\".format(l=calculate_accuracy(y, A, w)))\n",
    "    return history\n",
    "\n",
    "history_cd = coordinate_descent_for_svm_demo(y, A, trace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare it with SGD on original problem for the SVM. In this part, you will implement stochastic gradient descent on the primal SVM objective. The stochasticity comes from sampling data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient_svm(A_sample, b_sample, lambda_, w_t, num_data_points):\n",
    "    \"\"\"\n",
    "    Calculate stochastic gradient over A_batch, b_batch.\n",
    "    A_sample: A data sample, shape=(num_features)\n",
    "    b_sample: Corresponding +1 or -1 label, scalar\n",
    "    w_t: Model weights, shape=(num_features)\n",
    "    num_data_points: Total size of the dataset, scalar integer\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_svm_demo(A, b, gamma, batch_size=1, trace=False):\n",
    "    history = defaultdict(list) if trace else None\n",
    "    num_data_points, num_features = np.shape(A)\n",
    "    max_iter = 1000000\n",
    "    lambda_ = 0.01\n",
    "    \n",
    "    w_t = np.zeros(num_features)\n",
    "    \n",
    "    current_iter = 0\n",
    "    while (current_iter < max_iter):\n",
    "        i = random.randint(0,num_data_points - 1)\n",
    "        b_batch, A_batch = b[i], A[i]\n",
    "        gradient = compute_stoch_gradient_svm(A_batch, b_batch, lambda_, w_t, num_data_points)\n",
    "        w_t = w_t - gamma * gradient\n",
    "        if current_iter % 100000 == 0:\n",
    "            primal_value = calculate_primal_objective(y, A, w_t, lambda_)\n",
    "            print('iteration=%i, primal:%.5f'%(\n",
    "                    current_iter, primal_value))\n",
    "        if current_iter % 1000 == 0:\n",
    "            primal_value = calculate_primal_objective(y, A, w_t, lambda_)\n",
    "            if trace:\n",
    "                history['objective_function'].append(primal_value)\n",
    "                history['iter'].append(current_iter)\n",
    "        current_iter += 1\n",
    "    print(\"training accuracy = {l}\".format(l=calculate_accuracy(y, A, w_t)))\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different stepsized and find the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare SGD with Coordinate Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare two algorithms in terms of convergence, time complexities per iteration. Which one is easier to use?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
